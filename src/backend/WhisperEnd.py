from fastapi import FastAPI, Form, UploadFile, File
import shutil
import librosa
import os
import uvicorn
import gc

import time
import logging
from pathlib import Path

current_file_path = Path(__file__).resolve()

project_root = current_file_path.parent.parent.parent

model_root = project_root / "model"
log_root = project_root / "log"

log_root.mkdir(parents=True, exist_ok=True) # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º

logging.basicConfig(
    filename=str(log_root / "performance.log"), 
    level=logging.INFO, 
    format='%(asctime)s - %(message)s'
    )

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoProcessor
from transformers import WhisperConfig, WhisperForConditionalGeneration, WhisperProcessor
import torch
from peft import PeftModel

device = "cuda" if torch.cuda.is_available() else "cpu"

BASE_MODEL_PATH = {
    "asr": str(model_root / "asr" / "whisper-large"), 
    "mt": str(model_root / "mt" / "nllb-200-distilled-600M")
}

MODEL_PATH = {
    "asr": {
        "whisper-large": str(model_root / "asr" / "whisper-large"),
        "whisper-large-finetune": str(model_root / "asr" / "whisper-large-finetune")
        },
    "mt": {
        "nllb-200-distilled-600M": str(model_root / "mt" / "nllb-200-distilled-600M"),
        "nllb-200-distilled-600M-finetune": str(model_root / "mt" / "nllb-200-distilled-600M-finetune")
    }
}

global_models = {
    "asr": {
        "model": None,
        # "configuration": None,
        "processor": None,
        "current_type": None
        },
    "mt": {
        "model": None,
        "tokenizer": None,
        "current_type": None
    }
}

# åŠ è½½asræ¨¡å‹
def load_asr(model_type: str):
    """åˆ‡æ¢asræ¨¡å‹

    Args:
        model_type (str): éœ€è¦çš„asræ¨¡å‹

    Returns:
        (bool): æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
        
        (str): åŠ è½½çŠ¶æ€/å¤±è´¥åŸå› 
    """
    if global_models["asr"]["current_type"] == model_type:
        return True, "asræ¨¡å‹å·²åŠ è½½å®Œæˆ"
    print(f"[ASR] æ­£åœ¨åŠ è½½ {model_type} æ¨¡å‹")
    
    if global_models["asr"]["model"] is not None:
        del global_models["asr"]["model"]
        # del global_models["asr"]["configuration"]
        del global_models["asr"]["processor"]
        global_models["asr"]["model"] = None
        # global_models["asr"]["configuration"] = None
        global_models["asr"]["processor"] = None
        
    gc.collect()
    torch.cuda.empty_cache()
    
    try:
        base_model_path = BASE_MODEL_PATH["asr"]
        processor = WhisperProcessor.from_pretrained(base_model_path)
        base_model = WhisperForConditionalGeneration.from_pretrained(base_model_path).to(device)
        base_model.generation_config.forced_decoder_ids = None
        
        model_path = MODEL_PATH["asr"][model_type]
        if os.path.exists(os.path.join(model_path, "adapter_config.json")):
            print(f"[ASR] æ£€æµ‹åˆ° LoRA Adapter, æ­£åœ¨æŒ‚è½½å¾®è°ƒæƒé‡")
            # ä½¿ç”¨ PeftModel åŠ è½½ Adapter
            peft_model = PeftModel.from_pretrained(base_model, model_path)
            
            model = peft_model.merge_and_unload()
        else:
            # å¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼ˆæ²¡æœ‰ adapter æ–‡ä»¶ï¼‰ï¼Œç›´æ¥ç”¨ base_model å˜é‡å³å¯
            # ä½†é’ˆå¯¹ä½ çš„æˆªå›¾ï¼Œè‚¯å®šä¼šèµ°ä¸Šé¢é‚£ä¸ª if åˆ†æ”¯
            model = base_model
        # global_models["asr"]["configuration"] = WhisperConfig.from_pretrained(model_path)
        global_models["asr"]["processor"] = processor
        global_models["asr"]["model"] = model
        global_models["asr"]["current_type"] = model_type
        print(f"[ASR] åŠ è½½å®Œæˆ")
        return True, "success"
    except Exception as e:
        print(f"[ASR] åŠ è½½å¤±è´¥: {e}")
        return False, str(e)

# åŠ è½½mtæ¨¡å‹
def load_mt(model_type: str):
    """åˆ‡æ¢mtæ¨¡å‹

    Args:
        model_type (str): éœ€è¦çš„asræ¨¡å‹

    Returns:
        (bool): æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
        
        (str): åŠ è½½çŠ¶æ€/å¤±è´¥åŸå› 
    """
    if global_models["mt"]["current_type"] == model_type:
        return True, "mtæ¨¡å‹å·²åŠ è½½å®Œæˆ"
    print(f"[MT] æ­£åœ¨åŠ è½½ {model_type} æ¨¡å‹")
    
    if global_models["mt"]["model"] is not None:
        del global_models["mt"]["model"]
        del global_models["mt"]["tokenizer"]
        global_models["mt"]["model"] = None
        global_models["mt"]["tokenizer"] = None
        
    gc.collect()
    torch.cuda.empty_cache()
    
    try:
        base_model_path = BASE_MODEL_PATH["mt"]
        model_path = MODEL_PATH["mt"][model_type]
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_path).to(device)
        
        if os.path.exists(os.path.join(model_path, "adapter_config.json")):
            print(f"[MT] æ£€æµ‹åˆ° LoRA Adapter, æ­£åœ¨æŒ‚è½½å¾®è°ƒæƒé‡...")
            # ä½¿ç”¨ PeftModel åŠ è½½ Adapter
            peft_model = PeftModel.from_pretrained(base_model, model_path)
            
            model = peft_model.merge_and_unload()
        else:
            # å¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼ˆæ²¡æœ‰ adapter æ–‡ä»¶ï¼‰ï¼Œç›´æ¥ç”¨ base_model å˜é‡å³å¯
            # ä½†é’ˆå¯¹ä½ çš„æˆªå›¾ï¼Œè‚¯å®šä¼šèµ°ä¸Šé¢é‚£ä¸ª if åˆ†æ”¯
            model = base_model
            
        global_models["mt"]["tokenizer"] = tokenizer
        global_models["mt"]["model"] = model
        global_models["mt"]["current_type"] = model_type
        print(f"[MT] åŠ è½½å®Œæˆ")
        return True, "success"
    except Exception as e:
        print(f"[MT] åŠ è½½å¤±è´¥: {e}")
        return False, str(e)   

app = FastAPI()

@app.on_event("startup")
async def startup_event():
    # ã€å¯åŠ¨é€»è¾‘ (Startup)ã€‘: yield ä¹‹å‰
    print("ğŸš€ ç³»ç»Ÿå¯åŠ¨ä¸­... æ­£åœ¨åŠ è½½é»˜è®¤æ¨¡å‹")
    
    # åŠ è½½é»˜è®¤æ¨¡å‹ (å¾®è°ƒå‰)
    success_asr, msg_asr = load_asr("whisper-large")
    success_mt, msg_mt = load_mt("nllb-200-distilled-600M")
    
    if success_asr and success_mt:
        print("é»˜è®¤æ¨¡å‹åŠ è½½å®Œæ¯•")
    else:
        print(f"æ¨¡å‹åŠ è½½é”™è¯¯: ASR={msg_asr}, MT={msg_mt}")

    yield  # åº”ç”¨ç¨‹åºè¿è¡ŒæœŸé—´ï¼Œä»£ç ä¼šåœåœ¨è¿™é‡Œ

    print("ç³»ç»Ÿå…³é—­ï¼Œæ­£åœ¨æ¸…ç†æ˜¾å­˜")
    
    # æ¸…ç©ºå…¨å±€å¼•ç”¨
    global_models["asr"]["model"] = None
    # global_models["asr"]["configuration"] = None
    global_models["asr"]["processor"] = None
    global_models["mt"]["model"] = None
    global_models["mt"]["tokenizer"] = None
    
    gc.collect()
    torch.cuda.empty_cache()
    print("ç³»ç»Ÿç»“æŸ")

@app.post("/switch_asr")
async def api_switch_asr(model_name: str = Form(...)):
    """æ¥æ”¶å‚æ•°: asrmodel1 æˆ– asrmodel2"""
    success, msg = load_asr(model_name)
    if success:
        return {"status": "success", "current": model_name}
    else:
        return {"status": "error", "message": msg}

@app.post("/switch_mt")
async def api_switch_mt(model_name: str = Form(...)):
    """æ¥æ”¶å‚æ•°: mtmodel1 æˆ– mtmodel2"""
    success, msg = load_mt(model_name)
    if success:
        return {"status": "success", "current": model_name}
    else:
        return {"status": "error", "message": msg}

# @app.post("/asr")
# async def asr(
#     src_lang: str = Form(...), 
#     audio_file: UploadFile = File(...)
#     ):
#     model = global_models["asr"]["model"]
#     processor = global_models["asr"]["processor"]
#     # configuration = global_models["asr"]["configuration"]
    
#     if model is None:
#         return {"error": "[ASR] æ¨¡å‹æœªåŠ è½½"}
    
#     temp_filename = f"temp_{audio_file.filename}"
#     try:
#         with open(temp_filename, "wb") as b:
#             shutil.copyfileobj(audio_file.file, b)
#         lang_map = {
#             "ä¸­æ–‡": "chinese",
#             "ç²¤è¯­": "cantonese",
#             "è‹±è¯­": "english"
#         }
#         lang = lang_map.get(src_lang)
#         audio_array, _ = librosa.load(temp_filename, sr=16000)

#         # è®°å½•éŸ³é¢‘æ—¶é•¿å’Œå¼€å§‹æ—¶é—´
#         audio_time = librosa.get_duration(y=audio_array, sr=16000)
#         start_time = time.time()
        
#         inputs = processor(audio_array, sampling_rate=16000, return_tensors="pt")
#         input_features = inputs.input_features.to(device)
#         predicted_ids = model.generate(input_features, language=lang, task="transcribe")
#         transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
#         # è®¡ç®—æ¨ç†æ—¶é•¿ä»¥åŠRTF(Real Time Factor)
#         inference_time = time.time() - start_time
#         rtf = inference_time / audio_time
#         logging.info(f"[ASR] éŸ³é¢‘æ—¶é•¿: {audio_time:.2f}s, æ¨ç†æ—¶é•¿: {inference_time:.2f}, RTF: {rtf:.4f}")
        
#     finally:
#         if os.path.exists(temp_filename):
#             os.remove(temp_filename)
#     return{
#         "text": transcription,
#         "asr_performance":{
#             "audio_time": audio_time,
#             "inference_time": inference_time,
#             "RTF": rtf
#         }
#         }


@app.post("/asr")
async def asr(
    src_lang: str = Form(...), 
    audio_file: UploadFile = File(...)
    ):
    model = global_models["asr"]["model"]
    processor = global_models["asr"]["processor"]
    
    if model is None:
        return {"error": "[ASR] æ¨¡å‹æœªåŠ è½½"}
    
    temp_filename = f"temp_{audio_file.filename}"
    try:
        with open(temp_filename, "wb") as b:
            shutil.copyfileobj(audio_file.file, b)
        
        # 1. è¯­è¨€ä»£ç æ˜ å°„ä¿®æ­£
        # Whisper çš„ language å‚æ•°éœ€è¦çš„æ˜¯ç®€å•çš„ä»£ç ï¼Œæ¯”å¦‚ 'zh', 'en'
        # è€Œä¸æ˜¯ 'chinese', 'english' (è™½ç„¶ processor æœ‰æ—¶èƒ½è‡ªåŠ¨è¯†åˆ«ï¼Œä½†ç”¨æ ‡å‡†ä»£ç æœ€ç¨³)
        lang_map = {
            "ä¸­æ–‡": "Chinese",
            "ç²¤è¯­": "zh", # Whisper æ²¡æœ‰ä¸“é—¨çš„ç²¤è¯­ä»£ç ï¼Œé€šå¸¸å½’ç±»ä¸º zhï¼Œæˆ–è€…æ˜¯ model specific
            "è‹±è¯­": "English"
        }
        lang = lang_map.get(src_lang, "zh") # é»˜è®¤ zh

        audio_array, _ = librosa.load(temp_filename, sr=16000)
        audio_time = librosa.get_duration(y=audio_array, sr=16000)
        start_time = time.time()
        
        # 2. å¤„ç†è¾“å…¥ç‰¹å¾
        inputs = processor(audio_array, sampling_rate=16000, return_tensors="pt", return_attention_mask=True)
        input_features = inputs.input_features.to(device)
        
        # if "attention_mask" in inputs:
        #     attention_mask = inputs.attention_mask.to(device)
        # else:
        #     # å¯¹äº Whisperï¼Œé€šå¸¸ feature æ˜¯å®šé•¿çš„ (3000å¸§)ï¼Œmask å…¨ä¸º 1 å³å¯
        #     attention_mask = torch.ones(input_features.shape[0], input_features.shape[2], dtype=torch.long).to(device)
        
        # ã€æ ¸å¿ƒä¿®æ”¹ç‚¹ã€‘æ˜¾å¼è·å– forced_decoder_ids
        # è¿™æ ·æ—¢é¿å…äº† config é‡Œçš„å†²çªï¼Œåˆæ˜ç¡®å‘Šè¯‰äº†æ¨¡å‹ç”¨ä»€ä¹ˆè¯­è¨€å¼€å§‹
        forced_decoder_ids = processor.get_decoder_prompt_ids(
            language=lang, 
            task="transcribe"
        )
        
        # 3. ç”Ÿæˆé…ç½®ä¼˜åŒ– (é˜²æ­¢æ— é™å¾ªç¯)
        # no_repeat_ngram_size: é˜²æ­¢ "ä½ ä½ ä½ " è¿™ç§é‡å¤
        # max_new_tokens: å¼ºåˆ¶é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé˜²æ­¢ "å°ˆå°ˆå°ˆ..." æ— é™è¾“å‡º
        predicted_ids = model.generate(
            input_features,
            forced_decoder_ids=forced_decoder_ids,
            max_new_tokens=256,
            
            # ã€ä¿®å¤4ã€‘æŠ‘åˆ¶é‡å¤ç”Ÿæˆçš„å¼ºåŠ›å‚æ•°
            no_repeat_ngram_size=3,    # ç¦æ­¢è¿ç»­3ä¸ªå­—é‡å¤
            repetition_penalty=1.1,    # æƒ©ç½šé‡å¤å†…å®¹
            temperature=0.2,           # é™ä½éšæœºæ€§ï¼Œè®©è¾“å‡ºæ›´ç¨³å®š
            
            # ã€æ¶ˆé™¤è­¦å‘Šã€‘æ˜¾å¼å…³é—­ä¸éœ€è¦çš„è‡ªåŠ¨è®¾ç½®ï¼Œé˜²æ­¢ LogitsProcessor å†²çªè­¦å‘Š
            use_cache=True
        )
        
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
        # transcription = 
        
        inference_time = time.time() - start_time
        rtf = inference_time / audio_time
        logging.info(f"[ASR] éŸ³é¢‘æ—¶é•¿: {audio_time:.2f}s, æ¨ç†æ—¶é•¿: {inference_time:.2f}, RTF: {rtf:.4f}")
        
    finally:
        if os.path.exists(temp_filename):
            os.remove(temp_filename)
            
    return{
        "text": transcription,
        "asr_performance":{
            "audio_time": audio_time,
            "inference_time": inference_time,
            "RTF": rtf
        }
    }

@app.post("/mt")
async def mt(
    src_lang: str = Form(...),
    tar_lang: str = Form(...),
    text: str = Form(...)
):
    model = global_models["mt"]["model"]
    tokenizer = global_models["mt"]["tokenizer"]
    
    if model is None:
        return {"error": "[MT] æ¨¡å‹æœªåŠ è½½"}
    
    lang_map = {
        "ä¸­æ–‡": "zho_Hans",
        "ç²¤è¯­": "yue_Hant",
        "è‹±è¯­": "eng_Latn"
    }
    if not text:
        return {"error": "Text cannot be empty"}
    
    try:
        tokenizer.src_lang = lang_map[src_lang]
        inputs = tokenizer(text, return_tensors="pt").to(device)
        try:
            forced_bos_token_id = tokenizer.convert_tokens_to_ids(lang_map[tar_lang])
        except KeyError:
            return {"error": f"Unsupported target language code: {tar_lang}"}

        start_time = time.time()
        
        with torch.no_grad():
            generated_tokens = model.generate(
                **inputs, 
                forced_bos_token_id=forced_bos_token_id, 
                max_length=100  # æ ¹æ®éœ€è¦è°ƒæ•´æœ€å¤§é•¿åº¦
            )
        
        result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        
        # è®¡ç®—æ¨ç†æ—¶é—´ä»¥åŠTPS(Token/s)
        inference_time = time.time() - start_time
        token_nums = len(generated_tokens[0])
        tps = token_nums / inference_time
        logging.info(f"[MT] è¾“å…¥é•¿åº¦: {len(text)}, è¾“å‡ºTokené•¿åº¦: {token_nums}, æ¨ç†æ—¶é—´: {inference_time:.2f}s, TPS: {tps:.2f}")
        
        return {
            "translated_text": result,
            "mt_performance": {
                "input_length": len(text),
                "token_length": token_nums,
                "inference_time": inference_time,
                "token_per_sec": tps
            }
            }

    except Exception as e:
        return {"error": f"Generation failed: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=6006)

